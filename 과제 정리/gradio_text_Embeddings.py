# -*- coding: utf-8 -*-
from dotenv import load_dotenv
load_dotenv()

import gradio as gr
import numpy as np
import os
import pandas as pd
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.preprocessing import normalize
import re
import matplotlib.pyplot as plt
import fitz  # PyMuPDF
import warnings

from langchain.text_splitter import RecursiveCharacterTextSplitter

# Suppress specific warnings
warnings.filterwarnings("ignore", category=UserWarning, module='gradio.components.dropdown')


# --- 1. ë°ì´í„° ì†ŒìŠ¤ ì •ì˜ ---
def get_sample_data():
    """ë‚´ì¥ ìƒ˜í”Œ ë°ì´í„°ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    return [
        {"category": "ì¼ìƒ", "text": "ì˜¤ëŠ˜ ë‚ ì”¨ê°€ ì •ë§ ì¢‹ë„¤ìš”."},
        {"category": "ì¼ìƒ", "text": "ì ì‹¬ìœ¼ë¡œ ë¬´ì—‡ì„ ë¨¹ì„ê¹Œìš”?"},
        {"category": "ì¼ìƒ", "text": "ì €ëŠ” ì§€ê¸ˆ ì»¤í”¼ë¥¼ ë§ˆì‹œê³  ìˆì–´ìš”."},
        {"category": "ì¼ìƒ", "text": "What a beautiful day!"},
        {"category": "IT", "text": "ìµœì‹  AI ëª¨ë¸ì˜ ì„±ëŠ¥ì´ ë†€ëìŠµë‹ˆë‹¤."},
        {"category": "IT", "text": "íŒŒì´ì¬ì€ ë°°ìš°ê¸° ì‰¬ìš´ í”„ë¡œê·¸ë˜ë° ì–¸ì–´ì…ë‹ˆë‹¤."},
        {"category": "IT", "text": "ì´ ë²„ê·¸ëŠ” ì–´ë–»ê²Œ í•´ê²°í•´ì•¼ í• ê¹Œìš”?"},
        {"category": "IT", "text": "The new API is much faster."},
        {"category": "ë¹„ì¦ˆë‹ˆìŠ¤", "text": "ë‹¤ìŒ ë¶„ê¸° ì‹¤ì  ë°œí‘œê°€ ê¸°ëŒ€ë©ë‹ˆë‹¤."},
        {"category": "ë¹„ì¦ˆë‹ˆìŠ¤", "text": "ìƒˆë¡œìš´ ë§ˆì¼€íŒ… ì „ëµì´ í•„ìš”í•©ë‹ˆë‹¤."},
        {"category": "ë¹„ì¦ˆë‹ˆìŠ¤", "text": "ê³„ì•½ì„œ ê²€í† ë¥¼ ì™„ë£Œí–ˆìŠµë‹ˆë‹¤."},
        {"category": "ë¹„ì¦ˆë‹ˆìŠ¤", "text": "We need to increase our market share."},
        {"category": "ê³¼í•™", "text": "ìš°ì£¼ì˜ ì‹ ë¹„ëŠ” ëì´ ì—†ìŠµë‹ˆë‹¤."},
        {"category": "ê³¼í•™", "text": "ìƒˆë¡œìš´ ë…¼ë¬¸ì´ ë„¤ì´ì²˜ì— ê²Œì¬ë˜ì—ˆìŠµë‹ˆë‹¤."},
        {"category": "ê³¼í•™", "text": "ì–‘ìì—­í•™ì€ ë§¤ìš° í¥ë¯¸ë¡œìš´ ë¶„ì•¼ì…ë‹ˆë‹¤."},
        {"category": "ê³¼í•™", "text": "Photosynthesis is a complex process."},
        {"category": "ìŠ¤í¬ì¸ ", "text": "ì†í¥ë¯¼ ì„ ìˆ˜ê°€ ë˜ ê³¨ì„ ë„£ì—ˆìŠµë‹ˆë‹¤."},
        {"category": "ìŠ¤í¬ì¸ ", "text": "ì´ë²ˆ ì›”ë“œì»µì€ ì •ë§ ì¬ë¯¸ìˆë„¤ìš”."},
        {"category": "ìŠ¤í¬ì¸ ", "text": "ì €ëŠ” ì•¼êµ¬ ë³´ëŠ” ê²ƒì„ ì¢‹ì•„í•©ë‹ˆë‹¤."},
        {"category": "ìŠ¤í¬ì¸ ", "text": "The crowd went wild after the touchdown."},
        {"category": "ì˜ˆìˆ ", "text": "ì´ ê·¸ë¦¼ì€ ì •ë§ ì•„ë¦„ë‹µìŠµë‹ˆë‹¤."},
        {"category": "ì˜ˆìˆ ", "text": "ì €ëŠ” í´ë˜ì‹ ìŒì•…ì„ ì¦ê²¨ ë“£ìŠµë‹ˆë‹¤."},
        {"category": "ì˜ˆìˆ ", "text": "ì…°ìµìŠ¤í”¼ì–´ì˜ í¬ê³¡ì€ ì‹œëŒ€ë¥¼ ì´ˆì›”í•©ë‹ˆë‹¤."},
        {"category": "ì˜ˆìˆ ", "text": "Her voice is simply mesmerizing."},
        {"category": "ì—­ì‚¬", "text": "ì¡°ì„ ì™•ì¡°ì‹¤ë¡ì€ ìœ ë„¤ìŠ¤ì½” ì„¸ê³„ê¸°ë¡ìœ ì‚°ì…ë‹ˆë‹¤."},
        {"category": "ì—­ì‚¬", "text": "ë¡œë§ˆ ì œêµ­ì˜ ì—­ì‚¬ëŠ” ë§¤ìš° í¥ë¯¸ë¡­ìŠµë‹ˆë‹¤."},
        {"category": "ì—­ì‚¬", "text": "ì œ2ì°¨ ì„¸ê³„ëŒ€ì „ì€ ì¸ë¥˜ì—ê²Œ í° ìƒì²˜ë¥¼ ë‚¨ê²¼ìŠµë‹ˆë‹¤."},
        {"category": "ì—­ì‚¬", "text": "The Renaissance was a pivotal period in history."},
        {"category": "ê±´ê°•", "text": "ê·œì¹™ì ì¸ ìš´ë™ì€ ê±´ê°•ì— ì¢‹ìŠµë‹ˆë‹¤."},
        {"category": "ê±´ê°•", "text": "ì¶©ë¶„í•œ ìˆ˜ë©´ì„ ì·¨í•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤."},
        {"category": "ê±´ê°•", "text": "ë¹„íƒ€ë¯¼ CëŠ” ë©´ì—­ë ¥ ê°•í™”ì— ë„ì›€ì´ ë©ë‹ˆë‹¤."},
        {"category": "ê±´ê°•", "text": "A balanced diet is key to good health."},
    ]

def load_sentences(source_type, file_obj, use_chunking, chunk_size, chunk_overlap):
    """ë°ì´í„° ì†ŒìŠ¤ íƒ€ì…ì— ë”°ë¼ ë¬¸ì¥ ë¦¬ìŠ¤íŠ¸ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤."""
    if source_type == "ë‚´ì¥ ìƒ˜í”Œ":
        return get_sample_data()
    if file_obj is None:
        gr.Warning("íŒŒì¼ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.")
        return []

    file_path = file_obj.name
    _, file_extension = os.path.splitext(file_path)
    
    raw_text = ""
    try:
        if file_extension.lower() == ".pdf":
            doc = fitz.open(file_path)
            for page in doc:
                raw_text += page.get_text("text") + "\n"
            doc.close()
        elif file_extension.lower() == ".csv":
            # CSVëŠ” ì²­í‚¹ ëŒ€ìƒì´ ì•„ë‹ˆë¯€ë¡œ ê¸°ì¡´ ë¡œì§ ìœ ì§€
            df = pd.read_csv(file_path)
            sentences = []
            for _, row in df.iterrows():
                if 'text_kr' in row and pd.notna(row['text_kr']):
                    sentences.append({"category": row.get('category', 'CSV'), "text": row['text_kr']})
                if 'text_en' in row and pd.notna(row['text_en']):
                    sentences.append({"category": row.get('category', 'CSV'), "text": row['text_en']})
            return sentences
        else:
            gr.Warning(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹ì…ë‹ˆë‹¤: {file_extension}")
            return []

        # PDF ë˜ëŠ” ë‹¤ë¥¸ í…ìŠ¤íŠ¸ íŒŒì¼ì— ëŒ€í•œ ì²­í‚¹ ì²˜ë¦¬
        if use_chunking and raw_text and source_type == "PDF ì—…ë¡œë“œ":
            text_splitter = RecursiveCharacterTextSplitter(
                chunk_size=chunk_size,
                chunk_overlap=chunk_overlap
            )
            chunks = text_splitter.split_text(raw_text)
            return [{"category": "PDF-Chunked", "text": chunk} for chunk in chunks if chunk.strip()]
        elif raw_text:
            # ì²­í‚¹ ì‚¬ìš© ì•ˆ í•  ê²½ìš°, ê¸°ì¡´ì²˜ëŸ¼ ì¤„ë°”ê¿ˆìœ¼ë¡œ ë¶„ë¦¬
            lines = raw_text.split('\n')
            return [{"category": "PDF", "text": line.strip()} for line in lines if line.strip()]
        else:
            return []

    except Exception as e:
        gr.Error(f"íŒŒì¼ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return []


# --- 2. ì„ë² ë”© ëª¨ë¸ ë¡œë” ---
def get_embedder(model_name):
    """ì„ íƒëœ ëª¨ë¸ì— ëŒ€í•œ ì„ë² ë”© í•¨ìˆ˜/ê°ì²´ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    if model_name == "HuggingFace (multilingual-e5-large-instruct)":
        from langchain_community.embeddings import HuggingFaceEmbeddings
        return HuggingFaceEmbeddings(model_name="intfloat/multilingual-e5-large-instruct")

    elif model_name == "OpenAI (text-embedding-3-small)":
        if not os.environ.get("OPENAI_API_KEY"):
            gr.Warning("OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return None
        from langchain_openai import OpenAIEmbeddings
        return OpenAIEmbeddings(model="text-embedding-3-small")

    elif model_name == "Upstage (solar-embedding-1-large)":
        if not os.environ.get("UPSTAGE_API_KEY"):
            gr.Warning("UPSTAGE_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return None
        from langchain_upstage import UpstageEmbeddings
        # UpstageëŠ” ì§ˆë¬¸/ë¬¸ì„œ ëª¨ë¸ì´ ë¶„ë¦¬ë˜ì–´ ìˆìœ¼ë¯€ë¡œ íŠœí”Œë¡œ ë°˜í™˜
        query_embedder = UpstageEmbeddings(model="solar-embedding-1-large-query")
        passage_embedder = UpstageEmbeddings(model="solar-embedding-1-large-passage")
        return (query_embedder, passage_embedder)

    elif model_name == "Ollama (nomic-embed-text)":
        from langchain_ollama import OllamaEmbeddings
        return OllamaEmbeddings(model="nomic-embed-text")

    else:
        gr.Error("ì•Œ ìˆ˜ ì—†ëŠ” ëª¨ë¸ì…ë‹ˆë‹¤.")
        return None

# --- 3. ì „ì²˜ë¦¬ ë° ìœ ì‚¬ë„ ê³„ì‚° ---
def preprocess_text(text, options):
    """í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ ì˜µì…˜ì„ ì ìš©í•©ë‹ˆë‹¤."""
    if "ì†Œë¬¸ìí™”" in options:
        text = text.lower()
    if "ìˆ«ì/ê¸°í˜¸ ì œê±°" in options:
        text = re.sub(r'[^\w\s]', '', text)
    if "ì¤‘ë³µ ê³µë°± ì •ë¦¬" in options:
        text = re.sub(r'\s+', ' ', text).strip()
    return text

def calculate_similarity(query, data, model_name, top_k, threshold, preprocess_options):
    """ìœ ì‚¬ë„ ê³„ì‚°ì˜ ë©”ì¸ ë¡œì§"""
    if not query:
        gr.Warning("ê¸°ì¤€ ë¬¸ì¥ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")
        return pd.DataFrame(), None, "ê¸°ì¤€ ë¬¸ì¥ì„ ì…ë ¥í•´ì£¼ì„¸ìš”."
    if not data:
        gr.Warning("ë¹„êµí•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return pd.DataFrame(), None, "ë¹„êµí•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."

    try:
        embedder = get_embedder(model_name)
        if embedder is None:
            raise ValueError(f"{model_name} ëª¨ë¸ì„ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. API í‚¤ ë“±ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
    except Exception as e:
        gr.Error(f"ì„ë² ë”© ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
        return pd.DataFrame(), None, f"ì„ë² ë”© ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}"

    processed_query = preprocess_text(query, preprocess_options)
    sentences = [item['text'] for item in data]
    processed_sentences = [preprocess_text(s, preprocess_options) for s in sentences]

    try:
        # Upstage ëª¨ë¸ì˜ ê²½ìš° (query, passage) íŠœí”Œë¡œ ë°˜í™˜ë¨
        if isinstance(embedder, tuple):
            query_embedder, passage_embedder = embedder
            query_vec = query_embedder.embed_query(processed_query)
            doc_vecs = passage_embedder.embed_documents(processed_sentences)
        else: # ê·¸ ì™¸ ëª¨ë¸
            query_vec = embedder.embed_query(processed_query)
            doc_vecs = embedder.embed_documents(processed_sentences)
    except Exception as e:
        error_message = f"ì„ë² ë”© ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}"
        if "Connection refused" in str(e) and "Ollama" in model_name:
            error_message += "\n\nOllama ëª¨ë¸ì„ ì‚¬ìš©í•˜ë ¤ë©´ ë¡œì»¬ì— Ollama ì„œë¹„ìŠ¤ê°€ ì‹¤í–‰ ì¤‘ì´ì–´ì•¼ í•©ë‹ˆë‹¤."
        gr.Error(error_message)
        return pd.DataFrame(), None, error_message

    query_vec_normalized = normalize([query_vec], norm='l2')[0]
    doc_vecs_normalized = normalize(doc_vecs, norm='l2')
    sim_matrix = cosine_similarity([query_vec_normalized], doc_vecs_normalized)[0]

    results = []
    for i, score in enumerate(sim_matrix):
        if score >= threshold:
            results.append({
                "ìœ ì‚¬ë„": score,
                "ë¬¸ì¥": sentences[i],
                "ì¹´í…Œê³ ë¦¬": data[i]['category']
            })

    if not results:
        summary_message = f"ìœ ì‚¬ë„ ì„ê³„ì¹˜({threshold})ë¥¼ ë„˜ëŠ” ë¬¸ì¥ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ì„ê³„ì¹˜ë¥¼ ë‚®ì¶”ê±°ë‚˜ ë‹¤ë¥¸ ëª¨ë¸ì„ ì‚¬ìš©í•´ ë³´ì„¸ìš”."
        return pd.DataFrame(), None, summary_message

    df = pd.DataFrame(results).sort_values(by="ìœ ì‚¬ë„", ascending=False).head(top_k)
    if not df.empty:
        df.insert(0, "ìˆœìœ„", range(1, len(df) + 1))
        df['ìœ ì‚¬ë„'] = df['ìœ ì‚¬ë„'].map('{:.4f}'.format)

    fig = create_plot(df)
    summary = create_summary(df, model_name)

    return df, fig, summary

def create_plot(df):
    """ê²°ê³¼ ë°ì´í„°í”„ë ˆì„ì„ ë°›ì•„ ë§‰ëŒ€ ê·¸ë˜í”„ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
    if df.empty:
        return None
    fig, ax = plt.subplots()
    labels = [f"Rank {r}" for r in df['ìˆœìœ„']]
    scores = [float(s) for s in df['ìœ ì‚¬ë„']]
    ax.barh(labels, scores)
    ax.invert_yaxis()
    ax.set_xlabel('Similarity')
    ax.set_title('Top-K Similarity Scores')
    plt.tight_layout()
    return fig

def create_summary(df, model_name):
    """ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ê°„ë‹¨í•œ ìš”ì•½ ë¬¸êµ¬ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
    if df.empty:
        return f"[{model_name}] ëª¨ë¸ì— ëŒ€í•œ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤."

    top_category = df['ì¹´í…Œê³ ë¦¬'].iloc[0]
    category_concentration = (df['ì¹´í…Œê³ ë¦¬'] == top_category).sum() / len(df)

    summary_parts = []
    summary_parts.append(f"[{model_name}] ë¶„ì„ ê²°ê³¼:")
    summary_parts.append(f"- ê°€ì¥ ìœ ì‚¬í•œ ë¬¸ì¥ì€ '{df['ë¬¸ì¥'].iloc[0]}' (ìœ ì‚¬ë„: {df['ìœ ì‚¬ë„'].iloc[0]}) ì…ë‹ˆë‹¤.")
    summary_parts.append(f"- ìƒìœ„ {len(df)}ê°œ ì¤‘ {category_concentration:.0%}ê°€ '{top_category}' ì¹´í…Œê³ ë¦¬ì— ì†í•´, ëª¨ë¸ì´ ì¹´í…Œê³ ë¦¬ë¥¼ ì˜ êµ°ì§‘í™”í•˜ëŠ” ê²ƒìœ¼ë¡œ ë³´ì…ë‹ˆë‹¤.")
    
    return "\n".join(summary_parts)

# --- 4. Gradio UI ì •ì˜ ---

# ì‹œì‘ ì‹œ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ ìƒì„±
available_models = ["HuggingFace (multilingual-e5-large-instruct)", "Ollama (nomic-embed-text)"]
if os.environ.get("OPENAI_API_KEY"):
    available_models.append("OpenAI (text-embedding-3-small)")
if os.environ.get("UPSTAGE_API_KEY"):
    available_models.append("Upstage (solar-embedding-1-large)")

with gr.Blocks(theme=gr.themes.Soft()) as demo:
    gr.Markdown("# ë¬¸ì¥ ì„ë² ë”© ê¸°ë°˜ ìœ ì‚¬ë„ ë¹„êµ ë°ëª¨")
    gr.Markdown("ê¸°ì¤€ ë¬¸ì¥ê³¼ ì¤€ë¹„ëœ ë¬¸ì¥ ì§‘í•© ê°„ì˜ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ë¥¼ ê³„ì‚°í•˜ê³ , ëª¨ë¸ë³„ ê²°ê³¼ë¥¼ ë¹„êµí•©ë‹ˆë‹¤.\n" 
                "**ì°¸ê³ :** Ollama ëª¨ë¸ì€ ë¡œì»¬ì— [Ollama](https://ollama.com/)ê°€ ì„¤ì¹˜ ë° ì‹¤í–‰ë˜ì–´ ìˆì–´ì•¼ í•©ë‹ˆë‹¤.")

    with gr.Row():
        with gr.Column(scale=1):
            gr.Markdown("## â¬…ï¸ ì…ë ¥")
            query_input = gr.Textbox(lines=3, label="ê¸°ì¤€ ë¬¸ì¥", placeholder="ì—¬ê¸°ì— ë¹„êµí•  ë¬¸ì¥ì„ ì…ë ¥í•˜ì„¸ìš”...")
            gr.Examples(
                examples=[
                    "ê¹€ì¹˜ì— ëŒ€í•´ ì•Œë ¤ì¤˜",
                    "ë§ˆë¦¬ì˜¤ì— ëŒ€í•´ ì•Œë ¤ì¤˜",
                    "ì¼€ì´íŒ ì½˜ì„œíŠ¸ì˜ ì˜ë¯¸ëŠ” ë­ì•¼?",
                    "ì „ ì„¸ê³„ ì‚¬ëŒë“¤ì´ ì¦ê¸°ëŠ” ë¬¸í™”ëŠ” ë­ê°€ ìˆì–´?",
                ],
                inputs=query_input,
                label="ì˜ˆì‹œ ì§ˆë¬¸"
            )
            model_selector = gr.Dropdown(
                available_models,
                label="ì„ë² ë”© ëª¨ë¸ ì„ íƒ",
                value=available_models[0] if available_models else None,
                interactive=bool(available_models)
            )
            with gr.Row():
                top_k_slider = gr.Slider(1, 20, value=10, step=1, label="Top-K")
                threshold_slider = gr.Slider(0, 1, value=0.2, step=0.05, label="ìœ ì‚¬ë„ ì„ê³„ì¹˜")

            preprocess_options_checkbox = gr.CheckboxGroup(
                ["ì†Œë¬¸ìí™”", "ìˆ«ì/ê¸°í˜¸ ì œê±°", "ì¤‘ë³µ ê³µë°± ì •ë¦¬"],
                label="ì „ì²˜ë¦¬ ì˜µì…˜"
            )

            data_source_radio = gr.Radio(
                ["ë‚´ì¥ ìƒ˜í”Œ", "PDF ì—…ë¡œë“œ", "CSV ì—…ë¡œë“œ"],
                label="ë°ì´í„° ì†ŒìŠ¤",
                value="ë‚´ì¥ ìƒ˜í”Œ"
            )
            file_uploader = gr.File(label="íŒŒì¼ ì—…ë¡œë“œ", file_types=[".pdf", ".csv"], visible=False)

            with gr.Group(visible=False) as chunking_options:
                gr.Markdown("ğŸ“„ **ì²­í‚¹ ì˜µì…˜ (PDF ì „ìš©)**")
                use_chunking_checkbox = gr.Checkbox(label="ì—…ë¡œë“œ íŒŒì¼ì— ì²­í‚¹ ì ìš©", value=True)
                chunk_size_slider = gr.Slider(100, 2000, value=500, step=50, label="ì²­í¬ í¬ê¸° (Chunk Size)")
                chunk_overlap_slider = gr.Slider(0, 500, value=50, step=10, label="ì²­í¬ ì¤‘ì²© (Chunk Overlap)")

            run_button = gr.Button("ğŸš€ ìœ ì‚¬ë„ ê³„ì‚°", variant="primary")

        with gr.Column(scale=2):
            gr.Markdown("## â¡ï¸ ê²°ê³¼")
            result_table = gr.DataFrame(label="ìœ ì‚¬ë„ ìˆœìœ„")
            result_plot = gr.Plot(label="ìœ ì‚¬ë„ ì‹œê°í™”")
            summary_output = gr.Textbox(label="ê²°ê³¼ ìš”ì•½", lines=4, interactive=False)

    # --- 5. ì´ë²¤íŠ¸ í•¸ë“¤ëŸ¬ ì—°ê²° ---
    def on_source_change(source_type):
        is_upload = source_type in ["PDF ì—…ë¡œë“œ", "CSV ì—…ë¡œë“œ"]
        show_chunking = source_type == "PDF ì—…ë¡œë“œ"
        return {
            file_uploader: gr.update(visible=is_upload),
            chunking_options: gr.update(visible=show_chunking)
        }

    data_source_radio.change(fn=on_source_change, inputs=data_source_radio, outputs=[file_uploader, chunking_options])

    def run_analysis_wrapper(query, model, top_k, threshold, preprocess_opts, source_type, file_obj, use_chunking, chunk_size, chunk_overlap):
        data = load_sentences(source_type, file_obj, use_chunking, chunk_size, chunk_overlap)
        if not data:
            return pd.DataFrame(), None, "ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ì†ŒìŠ¤ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”."
        
        return calculate_similarity(query, data, model, top_k, threshold, preprocess_opts)

    run_button.click(
        fn=run_analysis_wrapper,
        inputs=[
            query_input, model_selector, top_k_slider, threshold_slider, 
            preprocess_options_checkbox, data_source_radio, file_uploader,
            use_chunking_checkbox, chunk_size_slider, chunk_overlap_slider
        ],
        outputs=[result_table, result_plot, summary_output]
    )


if __name__ == "__main__":
    if not available_models:
        print("ì˜¤ë¥˜: ì‚¬ìš© ê°€ëŠ¥í•œ ì„ë² ë”© ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤. API í‚¤ ë˜ëŠ” ë¡œì»¬ ëª¨ë¸ ì„¤ì •ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
    else:
        print(f"ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸: {available_models}")
    
    demo.launch()
